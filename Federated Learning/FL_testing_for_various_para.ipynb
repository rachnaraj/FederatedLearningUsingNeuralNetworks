{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg01zN90P4oC",
    "outputId": "bdd35bb0-7056-454f-9154-5f4bd7d3762f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# We only need to use it if we are taking inputs from google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOqq9Z6E5Hsc"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQitBa2n5Hsc",
    "outputId": "8fc0d83c-db61-4213-8993-8749ac1d4113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD # from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop # instead of from keras.optimizers import RMSprop\n",
    "# from keras import datasets\n",
    "\n",
    "# from keras.callbacks import LearningRateScheduler\n",
    "# from keras.callbacks import History\n",
    "\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# for saving model\n",
    "from keras.models import model_from_json\n",
    "import h5py\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oujHFKff5Hsd"
   },
   "source": [
    "### taking input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J04LDpAb5Hsd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Final plotting\n",
    "K = 25 # as we have 25 clients\n",
    "\n",
    "clients = {} # dictionary\n",
    "clients_cut = {}\n",
    "\n",
    "## reading the data from each client\n",
    "for st_code in range(101,126):\n",
    "    # clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\", index_col = 0)\n",
    "    clients[st_code] = pd.read_csv(\"(PATH_TO_INPUT_FOLDER)/dataNew/station_code\"+str(st_code)+\".csv\", index_col = 0) #for running on server\n",
    "\n",
    "    clients[st_code]['Measurement date'] = pd.to_datetime(clients[st_code]['Measurement date'])\n",
    "\n",
    "    clients[st_code]['year'] = clients[st_code]['Measurement date'].dt.year\n",
    "    clients[st_code]['month'] = clients[st_code]['Measurement date'].dt.month\n",
    "    clients[st_code]['week'] = clients[st_code]['Measurement date'].dt.week\n",
    "    clients[st_code]['day'] = clients[st_code]['Measurement date'].dt.day\n",
    "    clients[st_code]['hour'] = clients[st_code]['Measurement date'].dt.hour\n",
    "    clients[st_code]['minute'] = clients[st_code]['Measurement date'].dt.minute # minute is not significant; as only 0 values\n",
    "    clients[st_code]['dayOfWeek'] = clients[st_code]['Measurement date'].dt.dayofweek\n",
    "\n",
    "    # choosing features\n",
    "    clients[st_code] = clients[st_code][['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek', 'CO', \t'PM10',\t'PM2.5']]\n",
    "\n",
    "    clients[st_code].drop(clients[st_code][clients[st_code]['CO'] < 0].index, axis=0, inplace=True)\n",
    "    clients[st_code].drop(clients[st_code][clients[st_code]['PM10'] < 0].index, axis=0, inplace=True)\n",
    "    clients[st_code].drop(clients[st_code][clients[st_code]['PM2.5'] < 0].index, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# complete dataset\n",
    "frames = list(clients.values())\n",
    "dataset = pd.concat(frames)\n",
    "# display(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebWD0ZK45Hsn"
   },
   "source": [
    "## 4 hidden layer\n",
    "* with 64 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3D_lBr55Hsn"
   },
   "source": [
    "### traditional Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiBKwK2b5Hsn"
   },
   "outputs": [],
   "source": [
    "def traditional_ML_4L_64_train(loss_p, gas, train_data):\n",
    "  # initialize w_0\n",
    "  model = Sequential()\n",
    "\n",
    "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
    "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "  model.add(Dropout(0.1))\n",
    "\n",
    "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "  model.add(Dropout(0.1))\n",
    "\n",
    "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "  model.add(Dropout(0.1))\n",
    "\n",
    "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "  model.add(Dropout(0.1))\n",
    "\n",
    "  model.add(Dense(1, kernel_initializer= 'he_uniform', activation='linear')) # output layer\n",
    "\n",
    "\n",
    "  # compile the model\n",
    "  model.compile(loss='mse', \n",
    "              optimizer='adam', \n",
    "              metrics=['mse'])\n",
    "  \n",
    "  x_train, y_train = train_data[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  train_data[gas]\n",
    "\n",
    " \n",
    "\n",
    "  B = 1000 #int(len(x_train) * 0.01) # so for each client the batch size would be different depending upon the total sample size\n",
    "  E = 25 #max(30, B//10) # number of local epochs; it will also depend on sample size (indirectly)\n",
    "\n",
    "\n",
    "  model.fit(x_train, y_train, \n",
    "                  epochs = E, \n",
    "                  batch_size=B,\n",
    "                  verbose=False)\n",
    "  \n",
    "  return model\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdXXrL9n5Hso"
   },
   "outputs": [],
   "source": [
    "def traditional_ML_4L_64_test(model_name, gas, test_data):\n",
    "\n",
    "  ## loading the json and creating model\n",
    "  json_file = open(model_name + \".json\", 'r')\n",
    "  model_json = json_file.read()\n",
    "  json_file.close()\n",
    "\n",
    "  model = model_from_json(model_json)\n",
    "  model.load_weights(model_name + \".h5\")\n",
    "\n",
    "  ## need to compile the model again after loading\n",
    "  # compile the model\n",
    "  model.compile(loss='mse', \n",
    "              optimizer='adam', \n",
    "              metrics=['mse'])\n",
    "  \n",
    "\n",
    "  x_test, y_test = test_data[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  test_data[gas]\n",
    "\n",
    "  x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "  # adjusting the dimensions\n",
    "  x_test = x_test.reshape(x_test.shape[0], x_test.shape[1])\n",
    "  y_test = y_test.reshape(y_test.shape[0],1)\n",
    "  \n",
    "  y_predicted = model.predict(x_test)\n",
    "  \n",
    "\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  mse = mean_squared_error(y_predicted.ravel(), y_test.ravel())\n",
    "\n",
    "  NE = np.sum(abs(np.array(y_test) - np.array(y_predicted))) / np.sum(np.array(y_test))\n",
    "  return (mse, NE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hd3k_-mr5Hso"
   },
   "outputs": [],
   "source": [
    "# Final plotting\n",
    "K = 25 # as we have 25 clients\n",
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "loss_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "gases = ['CO', \t'PM10',\t'PM2.5']\n",
    "errors_tl_h4_64 = {'CO':[], \t'PM10':[],\t'PM2.5':[]}\n",
    "models = []\n",
    "model_name = \"TL_model_4L_64_\"\n",
    "\n",
    "\n",
    "# for gas in gases:\n",
    "#   for loss_p in loss_prob:\n",
    "#     error = traditional_ML_L3_32(loss_p, gas)\n",
    "#     errors_tl_h3_32[gas].append(error)\n",
    "\n",
    "for gas in gases:\n",
    "  for lp in loss_prob:\n",
    "      for st_code in range(101,126):\n",
    "          # splitting into train test split\n",
    "          X, y = clients[st_code][['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  clients[st_code][gas]\n",
    "          x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = lp, random_state=1)\n",
    "\n",
    "          # Normalization using min-max scaler\n",
    "          # Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n",
    "          # to avoid data leak\n",
    "          scaler_cols = x_train.columns\n",
    "          scaler_idx_train = x_train.index\n",
    "          scaler_idx_test = x_test.index\n",
    "\n",
    "          # from sklearn.preprocessing import MinMaxScaler\n",
    "          scaler = MinMaxScaler()\n",
    "\n",
    "          # transforming train data\n",
    "          x_train_encoded_scaled = pd.DataFrame(scaler.fit_transform(x_train), columns = scaler_cols, index=scaler_idx_train)\n",
    "          # display(x_train_encoded_scaled.head())\n",
    "\n",
    "          # transforming test data\n",
    "          x_test_encoded_scaled = pd.DataFrame(scaler.transform(x_test), columns = scaler_cols, index = scaler_idx_test)\n",
    "          # display(x_test_encoded_scaled.head())\n",
    "\n",
    "          # training data prep\n",
    "          if st_code == 101:\n",
    "            train_data = pd.concat([x_train_encoded_scaled, y_train], axis = 1)\n",
    "          else:\n",
    "            xy = pd.concat([x_train_encoded_scaled, y_train], axis=1)\n",
    "            train_data = pd.concat([train_data, xy], axis=0)\n",
    "          # clients_cut[st_code] = pd.concat([x_train_encoded_scaled, y_train], axis = 1)\n",
    "          \n",
    "\n",
    "          # testing data prep\n",
    "          # first appending x_test and y_test column wise\n",
    "          # then appending each clients' dataframe one below other\n",
    "          if st_code == 101:\n",
    "            test_data = pd.concat([x_test_encoded_scaled, y_test], axis = 1)\n",
    "          else:\n",
    "            xy = pd.concat([x_test_encoded_scaled, y_test], axis=1)\n",
    "            test_data = pd.concat([test_data, xy], axis=0)\n",
    "          # x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
    "      \n",
    "      # Training the model using traditional learning\n",
    "      model = traditional_ML_4L_64_train(lp, gas, train_data) #### COMMENT THIS LINE ONCE TRAINING AND SAVING OF MODEL IS DONE\n",
    "\n",
    "      ## Saving model\n",
    "      ## serializing model to JSON\n",
    "      model_address = \"(PUT LOCATION TO STORE THE MODELS, CREATE A Models FOLDER)/Models/\" + model_name + gas + str(lp) + \"_\" \n",
    "\n",
    "\n",
    "      #### COMMENT THIESE 3 LINES ONCE TRAINING AND SAVING OF MODEL IS DONE\n",
    "      model_json = model.to_json()\n",
    "      with open(model_address + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "      # # serialize weights to HDF5\n",
    "      model.save_weights(model_address + \".h5\") #### COMMENT THIS LINE ONCE TRAINING AND SAVING OF MODEL IS DONE\n",
    "\n",
    "\n",
    "      ## Testing model\n",
    "      error = traditional_ML_4L_64_test(model_address, gas, train_data)\n",
    "      errors_tl_h4_64[gas].append(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOyc0y785Hso"
   },
   "source": [
    "### Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGpTg3T75Hso"
   },
   "outputs": [],
   "source": [
    "# ClientUpdate(k, w): // Run on client k\n",
    "def Client_L4_64(client_idx, model, gas):\n",
    "    # B ← (split P_k into batches of size B) # what is p_K --> each clients' sample size\n",
    "    B = 1000 #int(len(clients[client_idx]) * 0.01) # so for each client the batch size would be different depending upon the total sample size\n",
    "    E = 25 #max(30, B//10) # number of local epochs; it will also depend on sample size (indirectly)\n",
    "\n",
    "    data = clients_cut[client_idx]\n",
    "    # data = dataset.loc[dataset['Station code'] == client_idx]\n",
    "\n",
    "    X, y = data[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  data[gas]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state=1)\n",
    "\n",
    "    try:\n",
    "        model.fit(x_train, y_train, \n",
    "                    epochs = E, \n",
    "                    batch_size=B,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"error\")\n",
    "\n",
    "    # return w to server\n",
    "    # in our case no need for any explicit return; as the model is pass by value\n",
    "    return data.shape[0] # this will return the number of total sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiEQKnru5Hsp"
   },
   "outputs": [],
   "source": [
    "# Server executes: \n",
    "def Server_L4_64(gas, T):\n",
    "    # initialize w_0\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer= 'he_uniform', activation='linear')) # output layer\n",
    "\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='mse', \n",
    "                optimizer='adam', \n",
    "                metrics=['mse'])\n",
    "    \n",
    "\n",
    "# for each round t = 1, 2, . . . do\n",
    "    for t in range(1, T):\n",
    "        C = np.random.random(1)[0] # random number between 0 and 1.\n",
    "\n",
    "        # m ← max(C  K, 1)\n",
    "        m = max(int(C*K), 1) # m == random number of client selected\n",
    "        weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
    "        n_k = [None] * m # parameters for weighted sum \n",
    "\n",
    "\n",
    "        # S_t ← (random set of m clients)\n",
    "        # S = {} # dictionary\n",
    "        S_t = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
    "\n",
    "        # No need for below loop, as server don't need access to client data, server only need clients' number\n",
    "        # for client in m_clients:\n",
    "        #   S[client] = clients[client]\n",
    "\n",
    "        initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
    "        # for t= 1 to T, initial_weights would be t-1th's final weights\n",
    "\n",
    "        # for each client k ∈ S_t in parallel do\n",
    "        client_idx = 0\n",
    "        for client in S_t:\n",
    "            # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
    "            n_k[client_idx] = Client_L4_64(client, model, gas) # pass by reference for model\n",
    "            weight_t_plus_1[client_idx] = model.get_weights()\n",
    "            client_idx += 1\n",
    "\n",
    "            # setting weights back to initial weights\n",
    "            model.set_weights(initial_weights)\n",
    "\n",
    "        # finding the weighted sum\n",
    "        final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0] / sum(n_k))\n",
    "\n",
    "        for idx in range(1, m):\n",
    "          # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
    "          final_weights_t += np.array(weight_t_plus_1[idx]) * (n_k[idx]/ sum(n_k))\n",
    "\n",
    "        # setting the aggregated weights\n",
    "        model.set_weights(final_weights_t)\n",
    "\n",
    "\n",
    "#         print(model.get_weights())\n",
    "    # return error_clients(model, loss_p)) # THIS LINE WON'T BE THERE IN REAL SERVER\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqkGJ2Sx5Hsp"
   },
   "outputs": [],
   "source": [
    "def error_clients_L4_64(model_name, gas, test_data):\n",
    "\n",
    "    ## loading the json and creating model\n",
    "    json_file = open(model_name + \".json\", 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "\n",
    "    model = model_from_json(model_json)\n",
    "    model.load_weights(model_name + \".h5\")\n",
    "\n",
    "    ## need to compile the model again after loading\n",
    "    # compile the model\n",
    "    model.compile(loss='mse', \n",
    "                optimizer='adam', \n",
    "                metrics=['mse'])\n",
    "  \n",
    "    \n",
    "    X, y = test_data[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  test_data[gas]\n",
    "    \n",
    "    # adjusting the dimensions\n",
    "    x_test, y_test = X.to_numpy(), y.to_numpy()\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1])\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "    y_predicted = model.predict(x_test)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    # print(\"y_predicted\", y_predicted)\n",
    "    # print(\"y_predicted.ravel()\", y_predicted.ravel())\n",
    "    NE = np.sum(abs(np.array(y_test) - np.array(y_predicted))) / np.sum(np.array(y_test))\n",
    "\n",
    "    # mse = mean_squared_error(y_predicted.ravel(), y_test.ravel())\n",
    "    \n",
    "    return NE\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWn32EhU5Hsp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create an Empty DataFrame object\n",
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "loss_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "T_round = [15, 20, 25, 35, 50, 75, 100]\n",
    "models = []\n",
    "errors_fl_h4_64 = {'CO':[], \t'PM10':[],\t'PM2.5':[]}\n",
    "gases = ['CO', \t'PM10',\t'PM2.5']\n",
    "model_name = \"FL_model_1L_32_\"\n",
    "\n",
    "\n",
    "# reading all the clients and building model\n",
    "for gas in gases:\n",
    "  for lp in loss_prob:\n",
    "      for st_code in range(101,126):\n",
    "          # splitting into train test split\n",
    "          X, y = clients[st_code][['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  clients[st_code][gas]\n",
    "          x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = lp, random_state=1)\n",
    "\n",
    "          # Normalization using min-max scaler\n",
    "          # Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n",
    "          # to avoid data leak\n",
    "          scaler_cols = x_train.columns\n",
    "          scaler_idx_train = x_train.index\n",
    "          scaler_idx_test = x_test.index\n",
    "\n",
    "          # from sklearn.preprocessing import MinMaxScaler\n",
    "          scaler = MinMaxScaler()\n",
    "\n",
    "          # transforming train data\n",
    "          x_train_encoded_scaled = pd.DataFrame(scaler.fit_transform(x_train), columns = scaler_cols, index=scaler_idx_train)\n",
    "          # display(x_train_encoded_scaled.head())\n",
    "\n",
    "          # transforming test data\n",
    "          x_test_encoded_scaled = pd.DataFrame(scaler.transform(x_test), columns = scaler_cols, index = scaler_idx_test)\n",
    "          # display(x_test_encoded_scaled.head())\n",
    "\n",
    "          # training data prep\n",
    "          clients_cut[st_code] = pd.concat([x_train_encoded_scaled, y_train], axis = 1)\n",
    "\n",
    "          # testing data prep\n",
    "          # first appending x_test and y_test column wise\n",
    "          # then appending each clients' dataframe one below other\n",
    "          if st_code == 101:\n",
    "            test_data = pd.concat([x_test_encoded_scaled, y_test], axis = 1)\n",
    "          else:\n",
    "            xy = pd.concat([x_test_encoded_scaled, y_test], axis=1)\n",
    "            test_data = pd.concat([test_data, xy], axis=0)\n",
    "          # x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "      error = [] # empty array for each loss prob\n",
    "      \n",
    "      for T in T_round:\n",
    "        # Training the model using federated learning\n",
    "        model = Server_L4_64(gas,T) ## COMMENT IT ONCE THE MODEL TRAINING IS DONE\n",
    "\n",
    "        ## Saving model\n",
    "        ## serializing model to JSON\n",
    "        model_address = \"(PUT THE ADDRESS TO STORE MODELS)/Models/\" + model_name + gas + str(lp) + \"_\" + str(T) + \"_\" \n",
    "\n",
    "\n",
    "        ## COMMENT BELOW LINES ONCE THE MODEL IS SAVED\n",
    "        model_json = model.to_json()\n",
    "        with open(model_address + \".json\", \"w\") as json_file:\n",
    "          json_file.write(model_json)\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_address + \".h5\") ## COMMENT THIS LINE ONCE THE MODEL IS SAVED\n",
    "\n",
    "\n",
    "        ## Testing model\n",
    "        error.append(error_clients_L4_64(model_address, gas, test_data))\n",
    "\n",
    "      # append the errors for each loss prob\n",
    "      errors_fl_h4_64[gas].append(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84QVPTZ95Hsp"
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74u7YoR25Hsq"
   },
   "source": [
    "#### Original plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt9xCv45gObP"
   },
   "outputs": [],
   "source": [
    "error_fl_CO = np.array(errors_fl_h4_64['CO']).T\n",
    "error_fl_PM25 = np.array(errors_fl_h4_64['PM2.5']).T\n",
    "error_fl_PM10 = np.array(errors_fl_h4_64['PM10']).T\n",
    "[15, 20, 25, 35, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TP7tOCgL5Hsq"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.lineplot(x = loss_prob, y = errors_tl, markers=True, dashes=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[0], markers=True,\n",
    "                  marker=\"o\", dashes=False, label = 'FL t:= 15', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[1], markers=True,\n",
    "                  marker=\"s\", dashes=False, label = 'FL t:= 20', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[2], markers=True,\n",
    "                  marker=\"P\", dashes=False, label = 'FL t:= 25', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[3], markers=True,\n",
    "                  marker=\"X\", dashes=False, label = 'FL t:= 35', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[4], markers=True,\n",
    "                  marker=\"v\", dashes=False, label = 'FL t:= 50', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[5], markers=True,\n",
    "                  marker=\"D\", dashes=False, label = 'FL t:= 75', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[6], markers=True,\n",
    "                  marker=\"|\", dashes=False, label = 'FL t:= 100', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.set(ylim=(0.2, 0.5))\n",
    "# ax.set(ylim=(0.0, 1.0))\n",
    "ax.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Loss Probability\", fontsize=14)\n",
    "plt.ylabel(\"NE Error\",fontsize=14)\n",
    "plt.title(\"PM2.5 [Architechture - 4HL/64Neuron]\") \n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z620QAKKlu7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.lineplot(x = loss_prob, y = errors_tl, markers=True, dashes=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[0], markers=True,\n",
    "                  marker=\"o\", dashes=False, label = 'FL t:= 15', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[1], markers=True,\n",
    "                  marker=\"s\", dashes=False, label = 'FL t:= 20', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[2], markers=True,\n",
    "                  marker=\"P\", dashes=False, label = 'FL t:= 25', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[3], markers=True,\n",
    "                  marker=\"X\", dashes=False, label = 'FL t:= 35', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[4], markers=True,\n",
    "                  marker=\"v\", dashes=False, label = 'FL t:= 50', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[5], markers=True,\n",
    "                  marker=\"D\", dashes=False, label = 'FL t:= 75', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[6], markers=True,\n",
    "                  marker=\"|\", dashes=False, label = 'FL t:= 100', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.set(ylim=(0.2, 0.5))\n",
    "# ax.set(ylim=(0.0, 1.0))\n",
    "ax.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Loss Probability\", fontsize=14)\n",
    "plt.ylabel(\"NE Error\",fontsize=14)\n",
    "plt.title(\"CO [Architechture - 4HL/64Neuron]\") \n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTqAijYPKUVU"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.lineplot(x = loss_prob, y = errors_tl, markers=True, dashes=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[0], markers=True,\n",
    "                  marker=\"o\", dashes=False, label = 'FL t:= 15', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[1], markers=True,\n",
    "                  marker=\"s\", dashes=False, label = 'FL t:= 20', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[2], markers=True,\n",
    "                  marker=\"P\", dashes=False, label = 'FL t:= 25', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[3], markers=True,\n",
    "                  marker=\"X\", dashes=False, label = 'FL t:= 35', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[4], markers=True,\n",
    "                  marker=\"v\", dashes=False, label = 'FL t:= 50', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[5], markers=True,\n",
    "                  marker=\"D\", dashes=False, label = 'FL t:= 75', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[6], markers=True,\n",
    "                  marker=\"|\", dashes=False, label = 'FL t:= 100', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.set(ylim=(0.2, 0.5))\n",
    "# ax.set(ylim=(0.0, 1.0))\n",
    "ax.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Loss Probability\", fontsize=14)\n",
    "plt.ylabel(\"NE Error\",fontsize=14)\n",
    "plt.title(\"PM10 [Architechture - 4HL/64Neuron]\") \n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmGLCekM4cRq"
   },
   "source": [
    "#### FL vs TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIVA6h8E3aNl"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 18))\n",
    "\n",
    "# plt.subplot(3,1,1)\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(errors_tl_h4_64['CO'])[:,1], markers=True, \n",
    "                  marker=\"D\", dashes=False, label = 'TL Error', linewidth=4, mfc='red', ms='8', mew='2')\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[0], markers=True,\n",
    "                  marker=\"o\", dashes=False, label = 'FL t:= 15', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[1], markers=True,\n",
    "                  marker=\"s\", dashes=False, label = 'FL t:= 20', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[2], markers=True,\n",
    "                  marker=\"P\", dashes=False, label = 'FL t:= 25', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[3], markers=True,\n",
    "                  marker=\"X\", dashes=False, label = 'FL t:= 35', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[4], markers=True,\n",
    "                  marker=\"v\", dashes=False, label = 'FL t:= 50', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[5], markers=True,\n",
    "                  marker=\"D\", dashes=False, label = 'FL t:= 75', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_CO)[6], markers=True,\n",
    "                  marker=\"|\", dashes=False, label = 'FL t:= 100', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "# ax.set(ylim=(0.2, 0.5))\n",
    "# ax.set(xlim=(0.1, 0.9))\n",
    "ax.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Loss Probability\", fontsize=14)\n",
    "plt.ylabel(\"NE Error\",fontsize=14)\n",
    "plt.title(\"CO - FL vs TL [Architechture - 1HL/32Neuron]\") \n",
    "plt.show(ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZGYE8O14e4K"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.lineplot(x = loss_prob, y = errors_tl, markers=True, dashes=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(errors_tl_h4_64['PM2.5'])[:,1], markers=True, \n",
    "                  marker=\"D\", dashes=False, label = 'TL Error', linewidth=4, mfc='red', ms='8', mew='2')\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[0], markers=True,\n",
    "                  marker=\"o\", dashes=False, label = 'FL t:= 15', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[1], markers=True,\n",
    "                  marker=\"s\", dashes=False, label = 'FL t:= 20', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[2], markers=True,\n",
    "                  marker=\"P\", dashes=False, label = 'FL t:= 25', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[3], markers=True,\n",
    "                  marker=\"X\", dashes=False, label = 'FL t:= 35', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[4], markers=True,\n",
    "                  marker=\"v\", dashes=False, label = 'FL t:= 50', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[5], markers=True,\n",
    "                  marker=\"D\", dashes=False, label = 'FL t:= 75', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM25)[6], markers=True,\n",
    "                  marker=\"|\", dashes=False, label = 'FL t:= 100', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.set(ylim=(0.2, 0.5))\n",
    "# ax.set(ylim=(0.0, 1.0))\n",
    "ax.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Loss Probability\", fontsize=14)\n",
    "plt.ylabel(\"NE Error\",fontsize=14)\n",
    "plt.title(\"PM2.5 [Architechture - 4HL/64Neuron]\") \n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Upzj3hsy5AjD"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.lineplot(x = loss_prob, y = errors_tl, markers=True, dashes=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(errors_tl_h4_64['PM2.5'])[:,1], markers=True, \n",
    "                  marker=\"D\", dashes=False, label = 'TL Error', linewidth=4, mfc='red', ms='8', mew='2')\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[0], markers=True,\n",
    "                  marker=\"o\", dashes=False, label = 'FL t:= 15', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[1], markers=True,\n",
    "                  marker=\"s\", dashes=False, label = 'FL t:= 20', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[2], markers=True,\n",
    "                  marker=\"P\", dashes=False, label = 'FL t:= 25', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[3], markers=True,\n",
    "                  marker=\"X\", dashes=False, label = 'FL t:= 35', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[4], markers=True,\n",
    "                  marker=\"v\", dashes=False, label = 'FL t:= 50', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[5], markers=True,\n",
    "                  marker=\"D\", dashes=False, label = 'FL t:= 75', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x = loss_prob, y = np.array(error_fl_PM10)[6], markers=True,\n",
    "                  marker=\"|\", dashes=False, label = 'FL t:= 100', linewidth=2,ms='10', mew='2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.set(ylim=(0.2, 0.5))\n",
    "# ax.set(ylim=(0.0, 1.0))\n",
    "ax.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Loss Probability\", fontsize=14)\n",
    "plt.ylabel(\"NE Error\",fontsize=14)\n",
    "plt.title(\"PM10 [Architechture - 4HL/64Neuron]\") \n",
    "plt.show(ax)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FL testing for various para.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
